{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1m1a4_JEhe55IKxrwxeAXvTi1WUclTi3q","timestamp":1665783275935},{"file_id":"1F_UYBpwl1cATWsLthvq-fFH5a6oH4vLd","timestamp":1636369985608},{"file_id":"1U3vnZeD8aiDg5Gh-SjnEyJyfrTHSRTkB","timestamp":1600620474551},{"file_id":"https://github.com/maryszmary/nlp-netology/blob/master/5/cnn_classification.ipynb","timestamp":1576678737162}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"z6ccN1AlFhNo"},"source":["## Text classification using CNN\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1U3vnZeD8aiDg5Gh-SjnEyJyfrTHSRTkB)"]},{"cell_type":"markdown","metadata":{"id":"oJ8_zAI8FhNp"},"source":["In this seminar we are going to build a CNN sentiment classifier using the IMDB review dataset. \n","\n","Materials source: https://github.com/bentrevett/pytorch-sentiment-analysis"]},{"cell_type":"markdown","metadata":{"id":"NHKLq9hEFhNr"},"source":["Assuming PyTorch is already installed, let's install additional modules and load the model for tokenization:"]},{"cell_type":"code","source":["!pip install torch==1.10.0+cu111"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fr-aP45JcaMN","executionInfo":{"status":"ok","timestamp":1665824882671,"user_tz":-180,"elapsed":2261,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"4b416dfe-899b-4d9d-8674-26a6f6b709fd"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.10.0+cu111 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for torch==1.10.0+cu111\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"GgGxeuQjG9NI"},"source":["# !pip3 install https://download.pytorch.org/whl/cpu/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5kiZro9eG-bG","executionInfo":{"status":"ok","timestamp":1665824895353,"user_tz":-180,"elapsed":4378,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}}},"source":["!pip install -q torchvision"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"a-WRgs-VFhNt","executionInfo":{"status":"ok","timestamp":1665824900594,"user_tz":-180,"elapsed":2903,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}}},"source":["!pip install -q torchtext"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"pB7uZ5L7FhN3","executionInfo":{"status":"ok","timestamp":1665824906333,"user_tz":-180,"elapsed":15,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}}},"source":["# !pip3 install spacy"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"QLGeXcUjFhN8","executionInfo":{"status":"ok","timestamp":1665824906336,"user_tz":-180,"elapsed":14,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}}},"source":["#!python3.6 -m spacy download en\n","# !python3 -m spacy download en_core_web_sm"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"QaPk16PLmgnw","executionInfo":{"status":"ok","timestamp":1665824906339,"user_tz":-180,"elapsed":15,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}}},"source":["# import spacy\n","#import en\n","# en_nlp = spacy.load('en_core_web_sm')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"2zclld6vmaH2","executionInfo":{"status":"ok","timestamp":1665824909186,"user_tz":-180,"elapsed":2415,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}}},"source":["import torch"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s4Yq-LOGmoOn","executionInfo":{"status":"ok","timestamp":1665824909188,"user_tz":-180,"elapsed":19,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"f58081fc-2166-4795-e6b2-403a94265fe7"},"source":["print(torch.__version__)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["1.12.1+cu113\n"]}]},{"cell_type":"code","metadata":{"id":"-ZlniJuAmp8F","executionInfo":{"status":"ok","timestamp":1665824913782,"user_tz":-180,"elapsed":519,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}}},"source":["SEED = 0\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BObMLa5mvQYM"},"source":["### Data"]},{"cell_type":"code","source":[],"metadata":{"id":"5aijB-dvbUtN","executionInfo":{"status":"ok","timestamp":1665824915581,"user_tz":-180,"elapsed":6,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C5Bipik6bWy9","executionInfo":{"status":"ok","timestamp":1665825064823,"user_tz":-180,"elapsed":148720,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"e26b443e-4fff-4d9f-e6a6-6883ed278c1c"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T-m8UH7pFGbw","executionInfo":{"status":"ok","timestamp":1665825072537,"user_tz":-180,"elapsed":4153,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"9d49821c-c195-431b-9593-31d58fcc686c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/drive/MyDrive/4 курс/NLP/ДЗ1/public_data (1).zip\n","  inflating: test_data.csv           \n","  inflating: train_data.csv          \n","  inflating: train_solution.csv      \n"]}],"source":["!unzip \"/content/drive/MyDrive/4 курс/NLP/ДЗ1/public_data (1).zip\""]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import re\n","from nltk.corpus import stopwords\n","import nltk \n","from nltk.tokenize import TweetTokenizer\n","pd.set_option('display.max_columns', None)  \n","pd.set_option('display.expand_frame_repr', False)\n","pd.set_option('max_colwidth', 800)\n","nltk.download(\"stopwords\")\n","tknzr = TweetTokenizer()\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IbjMCcK2FOJT","executionInfo":{"status":"ok","timestamp":1665825075760,"user_tz":-180,"elapsed":1504,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"715e673a-99f8-4cb4-d29f-59df0cf4766d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["df = pd.read_csv(\"train_data.csv\")\n","labels = pd.read_csv(\"train_solution.csv\")"],"metadata":{"id":"7cmshuzcFimD","executionInfo":{"status":"ok","timestamp":1665825075763,"user_tz":-180,"elapsed":11,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["df = df.merge(labels, left_on='id', right_on='id')\n"],"metadata":{"id":"Mj6D4xjHFk19","executionInfo":{"status":"ok","timestamp":1665825080510,"user_tz":-180,"elapsed":10,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FLGAJBxfFhN_"},"source":["Let's load the dataset and get a sample from it:"]},{"cell_type":"code","metadata":{"id":"7w84GaXumr0q","executionInfo":{"status":"ok","timestamp":1665825080512,"user_tz":-180,"elapsed":9,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}}},"source":["# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=17uuANm7Q1CunXHfTaF7IRY9Vy7qPl5_L' -O imdb.csv"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"wXTKRzx_00_c","executionInfo":{"status":"ok","timestamp":1665825081194,"user_tz":-180,"elapsed":24,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}}},"source":["# !head -n 3 imdb.csv"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":354},"id":"xsHAXknhFhOA","executionInfo":{"status":"ok","timestamp":1665825081196,"user_tz":-180,"elapsed":24,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"2d3489c2-e42d-444c-8d6f-12aef98c22db"},"source":["df.head()"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          message  category\n","0  271828  Over $616 million in Bitcoin was electrocated in September with Wrapped Bitcoin (WBTC) data from CoinDesk. The emissions increased by more than 160 per cent compared to August, when $232 million was currentized.  WBTC's output grew along with the demand for over-the-counter (OTC) market, reported by Grapefruit Trading. It became one of the first OTC platforms to release WBTC via the BitGo service.  The Grapefruit Trading Trader Getty Hill claims that clients' interest in translating bitcoin into WBTC comes from their desire to use the first cryptation in the decentralized finance ecosystem (DeFi).  CEO's FTX Exchange Sam Bankman-Frid acknowledged that the demand for WBTC at the OTC market remained significant, though not as high as it had been in previous months during the rapid rise o...         1\n","1  271829                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Quiz: Thursday or friday?         0\n","2  271830  The Australian Revenue Authority will start collecting taxes from crypto transactions this year. The Authority has developed methods to search for and identify cryptovaly investors.  In March 2018, the Australian Revenue Authority announced that the forthcoming identification process would involve AML and bilateral agreements. The data comparison method will also be used to deonymize buyers of crypthalates. Tax agent Liz Russell notes that most users will soon be identified and their debts will be known.  The authorities believe that, because of Bitcoin's recent strong appreciation to $200,000, investors could have earned profits that were not recorded in their tax obligations. If users have suffered losses, these amounts can be deducted from taxable profits.  The Authority recalls tha...         1\n","3  271831                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Let's continue😉. I present to you my new review          2\n","4  271832                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Here comes your future palette.         2"],"text/html":["\n","  <div id=\"df-1eddd715-6fa8-4b4a-b303-98602e7e3a2d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>message</th>\n","      <th>category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>271828</td>\n","      <td>Over $616 million in Bitcoin was electrocated in September with Wrapped Bitcoin (WBTC) data from CoinDesk. The emissions increased by more than 160 per cent compared to August, when $232 million was currentized.  WBTC's output grew along with the demand for over-the-counter (OTC) market, reported by Grapefruit Trading. It became one of the first OTC platforms to release WBTC via the BitGo service.  The Grapefruit Trading Trader Getty Hill claims that clients' interest in translating bitcoin into WBTC comes from their desire to use the first cryptation in the decentralized finance ecosystem (DeFi).  CEO's FTX Exchange Sam Bankman-Frid acknowledged that the demand for WBTC at the OTC market remained significant, though not as high as it had been in previous months during the rapid rise o...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>271829</td>\n","      <td>Quiz: Thursday or friday?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>271830</td>\n","      <td>The Australian Revenue Authority will start collecting taxes from crypto transactions this year. The Authority has developed methods to search for and identify cryptovaly investors.  In March 2018, the Australian Revenue Authority announced that the forthcoming identification process would involve AML and bilateral agreements. The data comparison method will also be used to deonymize buyers of crypthalates. Tax agent Liz Russell notes that most users will soon be identified and their debts will be known.  The authorities believe that, because of Bitcoin's recent strong appreciation to $200,000, investors could have earned profits that were not recorded in their tax obligations. If users have suffered losses, these amounts can be deducted from taxable profits.  The Authority recalls tha...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>271831</td>\n","      <td>Let's continue😉. I present to you my new review</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>271832</td>\n","      <td>Here comes your future palette.</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1eddd715-6fa8-4b4a-b303-98602e7e3a2d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1eddd715-6fa8-4b4a-b303-98602e7e3a2d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1eddd715-6fa8-4b4a-b303-98602e7e3a2d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["df.drop(['id'],axis=1,inplace=True)"],"metadata":{"id":"VXgiVVOrb4lv","executionInfo":{"status":"ok","timestamp":1665825081201,"user_tz":-180,"elapsed":23,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3WAUUMcixIRP"},"source":["### Write data to compatable structures"]},{"cell_type":"code","source":["df.to_csv(\"imdb.csv\", index=False)"],"metadata":{"id":"FlAOvruaeq_s","executionInfo":{"status":"ok","timestamp":1665825081205,"user_tz":-180,"elapsed":26,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"9oi2ZhEE1d9B","executionInfo":{"status":"error","timestamp":1665825081995,"user_tz":-180,"elapsed":15,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"d492e6db-186b-4ee5-df8f-df99e2c52f54"},"execution_count":20,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-e4634d7ebc4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTabularDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBucketIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext.legacy'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["from torchtext import data, datasets\n","from torchtext.vocab import Vocab"],"metadata":{"id":"rpYjPVym6ngF","executionInfo":{"status":"ok","timestamp":1665825159370,"user_tz":-180,"elapsed":510,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"yT8eYs4dxWW1","executionInfo":{"status":"error","timestamp":1665825086027,"user_tz":-180,"elapsed":12,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"colab":{"base_uri":"https://localhost:8080/","height":353},"outputId":"e6e49db9-91be-414f-f460-75bc4c81644a"},"source":["from torchtext import data\n","# data.Field is obsolete now\n","from torchtext.legacy import torchtext.legacy.data.Field"],"execution_count":21,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-914f5b7dcebc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# data.Field is obsolete now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext.legacy'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["import torchtext.legacy as torchtext\n"],"metadata":{"id":"ZuKB_nXi7R44","executionInfo":{"status":"error","timestamp":1665825318783,"user_tz":-180,"elapsed":4,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"2fd36389-6856-4490-a3ee-b98fa3901ffa","colab":{"base_uri":"https://localhost:8080/","height":317}},"execution_count":27,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-ddf098a5f0c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext.legacy'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["import torchtext"],"metadata":{"id":"VwV9B1Ib7Lsg","executionInfo":{"status":"ok","timestamp":1665825300299,"user_tz":-180,"elapsed":10,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["torchtext.legacy.data.Field"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171},"id":"pvvdHFzR7JAx","executionInfo":{"status":"error","timestamp":1665825303010,"user_tz":-180,"elapsed":4,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"112731fa-c391-4467-8138-6af16006ccbb"},"execution_count":26,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-74422200d4f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: module 'torchtext' has no attribute 'legacy'"]}]},{"cell_type":"code","metadata":{"id":"Ck6B9K1ayX7A","executionInfo":{"status":"error","timestamp":1665825172879,"user_tz":-180,"elapsed":541,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"colab":{"base_uri":"https://localhost:8080/","height":244},"outputId":"4cfdeb42-6cf4-4bc9-88b8-daef44b8a62e"},"source":["# Field and LabelField classes are responsible for the way data will be stored and processed\n","TEXT = data.Field(tokenize='spacy') # we'll use spacy for tokenization here\n","LABEL = data.LabelField()\n","\n","ds = data.TabularDataset(\n","  path='imdb.csv', format='csv',\n","  skip_header=True,\n","  fields=[('message', TEXT),\n","        ('category', LABEL)]\n",")"],"execution_count":23,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-8d92741623ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Field and LabelField classes are responsible for the way data will be stored and processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mTEXT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spacy'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# we'll use spacy for tokenization here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mLABEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLabelField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ds = data.TabularDataset(\n","\u001b[0;31mAttributeError\u001b[0m: module 'torchtext.data' has no attribute 'Field'"]}]},{"cell_type":"markdown","metadata":{"id":"MdpfQN9qOu0p"},"source":["ds - dataset - iterates through our texts & labels.\n","\n","**NB**: original column names don't matter since we pass column names to the `fields` argument."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O8-CSOf1xHuV","executionInfo":{"status":"ok","timestamp":1665822855884,"user_tz":-180,"elapsed":285,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"90c707c0-093b-4d8e-ec29-91cc98b84b33"},"source":["next(ds.message)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Over',\n"," '$',\n"," '616',\n"," 'million',\n"," 'in',\n"," 'Bitcoin',\n"," 'was',\n"," 'electrocated',\n"," 'in',\n"," 'September',\n"," 'with',\n"," 'Wrapped',\n"," 'Bitcoin',\n"," '(',\n"," 'WBTC',\n"," ')',\n"," 'data',\n"," 'from',\n"," 'CoinDesk',\n"," '.',\n"," 'The',\n"," 'emissions',\n"," 'increased',\n"," 'by',\n"," 'more',\n"," 'than',\n"," '160',\n"," 'per',\n"," 'cent',\n"," 'compared',\n"," 'to',\n"," 'August',\n"," ',',\n"," 'when',\n"," '$',\n"," '232',\n"," 'million',\n"," 'was',\n"," 'currentized',\n"," '.',\n"," ' ',\n"," 'WBTC',\n"," \"'s\",\n"," 'output',\n"," 'grew',\n"," 'along',\n"," 'with',\n"," 'the',\n"," 'demand',\n"," 'for',\n"," 'over',\n"," '-',\n"," 'the',\n"," '-',\n"," 'counter',\n"," '(',\n"," 'OTC',\n"," ')',\n"," 'market',\n"," ',',\n"," 'reported',\n"," 'by',\n"," 'Grapefruit',\n"," 'Trading',\n"," '.',\n"," 'It',\n"," 'became',\n"," 'one',\n"," 'of',\n"," 'the',\n"," 'first',\n"," 'OTC',\n"," 'platforms',\n"," 'to',\n"," 'release',\n"," 'WBTC',\n"," 'via',\n"," 'the',\n"," 'BitGo',\n"," 'service',\n"," '.',\n"," ' ',\n"," 'The',\n"," 'Grapefruit',\n"," 'Trading',\n"," 'Trader',\n"," 'Getty',\n"," 'Hill',\n"," 'claims',\n"," 'that',\n"," 'clients',\n"," \"'\",\n"," 'interest',\n"," 'in',\n"," 'translating',\n"," 'bitcoin',\n"," 'into',\n"," 'WBTC',\n"," 'comes',\n"," 'from',\n"," 'their',\n"," 'desire',\n"," 'to',\n"," 'use',\n"," 'the',\n"," 'first',\n"," 'cryptation',\n"," 'in',\n"," 'the',\n"," 'decentralized',\n"," 'finance',\n"," 'ecosystem',\n"," '(',\n"," 'DeFi',\n"," ')',\n"," '.',\n"," ' ',\n"," 'CEO',\n"," \"'s\",\n"," 'FTX',\n"," 'Exchange',\n"," 'Sam',\n"," 'Bankman',\n"," '-',\n"," 'Frid',\n"," 'acknowledged',\n"," 'that',\n"," 'the',\n"," 'demand',\n"," 'for',\n"," 'WBTC',\n"," 'at',\n"," 'the',\n"," 'OTC',\n"," 'market',\n"," 'remained',\n"," 'significant',\n"," ',',\n"," 'though',\n"," 'not',\n"," 'as',\n"," 'high',\n"," 'as',\n"," 'it',\n"," 'had',\n"," 'been',\n"," 'in',\n"," 'previous',\n"," 'months',\n"," 'during',\n"," 'the',\n"," 'rapid',\n"," 'rise',\n"," 'of',\n"," 'the',\n"," 'DeFi',\n"," 'projects',\n"," '.',\n"," 'At',\n"," 'the',\n"," 'same',\n"," 'time',\n"," ',',\n"," 'he',\n"," 'is',\n"," 'confident',\n"," 'that',\n"," 'even',\n"," 'if',\n"," 'deFi',\n"," \"'s\",\n"," 'holocaust',\n"," 'fails',\n"," ',',\n"," 'WBTC',\n"," 'will',\n"," 'be',\n"," 'more',\n"," 'popular',\n"," 'than',\n"," 'before',\n"," 'this',\n"," 'summer',\n"," '.',\n"," ' ',\n"," 'The',\n"," 'September',\n"," 'growth',\n"," 'is',\n"," 'also',\n"," 'due',\n"," 'to',\n"," 'the',\n"," 'activity',\n"," 'of',\n"," 'selected',\n"," 'large',\n"," 'players',\n"," 'who',\n"," 'have',\n"," 'grown',\n"," 'WBTCs',\n"," 'compared',\n"," 'to',\n"," 'August',\n"," '.',\n"," ' ',\n"," 'Overall',\n"," ',',\n"," 'the',\n"," 'volume',\n"," 'of',\n"," 'electronized',\n"," 'bitcoin',\n"," 'in',\n"," 'all',\n"," 'available',\n"," 'forms',\n"," ',',\n"," 'including',\n"," 'WBTC',\n"," ',',\n"," 'increased',\n"," 'by',\n"," '120',\n"," '%',\n"," 'in',\n"," 'September',\n"," 'to',\n"," 'more',\n"," 'than',\n"," '121,000',\n"," 'BTCs',\n"," 'or',\n"," '$',\n"," '1.3',\n"," 'billion',\n"," '.',\n"," 'In',\n"," 'August',\n"," ',',\n"," 'the',\n"," 'value',\n"," 'was',\n"," '55,000',\n"," 'BTC',\n"," '.',\n"," 'Small',\n"," 'projects',\n"," 'of',\n"," 'this',\n"," 'kind',\n"," 'continue',\n"," 'to',\n"," 'grow',\n"," '.',\n"," 'Hill',\n"," 'reported',\n"," 'that',\n"," 'the',\n"," 'customers',\n"," 'were',\n"," 'interested',\n"," 'in',\n"," 'renBTC',\n"," ',',\n"," 'tBTC',\n"," 'and',\n"," 'other',\n"," 'currents',\n"," ',',\n"," 'but',\n"," 'in',\n"," 'their',\n"," 'case',\n"," 'the',\n"," 'volumes',\n"," 'were',\n"," '\"',\n"," '100',\n"," 'per',\n"," 'cent',\n"," 'concentrated',\n"," 'in',\n"," 'WBTC',\n"," '\"',\n"," '.']"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"e9h0nnsi0Q0-","executionInfo":{"status":"ok","timestamp":1665822374146,"user_tz":-180,"elapsed":23,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"ce034ff7-b858-4068-caa2-02f6ac9bfee4"},"source":["next(ds.category)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"ECxpxH2m9VY8"},"source":["Build the dictionary and load embeddings.\n","\n","Taking into account the fact that there are 100K unique words in the collection, and the vectors are big, we will truncate the collection down to 25K words, and set the unk (unknown) token for all the other words.\n","\n","Torchtext has a repository with some of the vocabulary embeddings for English. `vectors =\" glove.6B.100d \"` means that in addition to building an index of words in the corpus, we will download and save the glove vectors from this repository."]},{"cell_type":"code","metadata":{"id":"9K_LTwEn78n5","executionInfo":{"status":"ok","timestamp":1665822584613,"user_tz":-180,"elapsed":195042,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"183a7f4a-057e-44c5-8b50-cc1b52a4a943"},"source":["TEXT.build_vocab(ds, max_size=25000, vectors=\"glove.6B.100d\")\n","LABEL.build_vocab(ds)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[".vector_cache/glove.6B.zip: 862MB [02:39, 5.42MB/s]                           \n","100%|█████████▉| 399999/400000 [00:15<00:00, 25058.87it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"d9omo_uS9P7V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665822910496,"user_tz":-180,"elapsed":256,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"682cf45a-1e8d-4ef5-e4e6-69e8b6db50e1"},"source":["# itos == i to s == index to string\n","print(TEXT.vocab.itos[:100])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['<unk>', '<pad>', ',', 'the', '.', 'of', 'to', 'a', 'and', 'in', ' ', \"'s\", 'I', 'that', 'is', 'it', 'for', 'on', 'The', '-', 'you', 'with', '\"', ')', 'be', '(', 'was', 'hey', \"n't\", 'will', 'not', 'are', 'from', 'by', '$', 'as', 'have', 'at', 'this', 'but', 'do', 'about', '!', ':', 'has', 'It', 'an', 'like', 'which', 'my', \"'m\", 'all', 'In', '?', 'can', \"'re\", 'we', 'more', 'time', 'or', 'na', 'gon', 'there', 'so', 'Bitcoin', '  ', 'its', 'million', 'been', 'new', 'they', 'your', 'no', 'up', 'market', 'one', \"'ve\", 'out', 'also', 'company', 'who', 'first', 'than', '%', 'me', 'their', 'if', 'According', 'he', 'were', 'now', 'what', 'had', 'year', 'digital', 'only', 'other', 'when', 'bitcoin', 'would']\n"]}]},{"cell_type":"code","metadata":{"id":"eZRu4ykFpz_q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665822938578,"user_tz":-180,"elapsed":281,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"06ea9d37-5e43-4a97-9fbe-8d50d28d2462"},"source":["# stoi == s to i == string to index\n","TEXT.vocab.stoi[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"AJl0ZOPBElSE"},"source":["Let's break down our dataset into training, validation (for parameters evaluation) and test."]},{"cell_type":"code","metadata":{"id":"6nuZQIq1FhOl"},"source":["train, val = ds.split() # default split is 0.7\n","val, test = val.split(split_ratio=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d4_u1nkyS740","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665822584616,"user_tz":-180,"elapsed":47,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"70030f92-f475-45a6-d85d-54a484eb1421"},"source":["print(len(train))\n","print(len(val))\n","print(len(test))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2691\n","576\n","577\n"]}]},{"cell_type":"code","source":["train."],"metadata":{"id":"YT-9Hz8OyQLS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eT-lY0stTbWo"},"source":["Now let's create batch iterators:"]},{"cell_type":"code","metadata":{"id":"lIJTX3ypTbrK"},"source":["BATCH_SIZE  = 128\n","\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train, val, test), \n","    batch_size=BATCH_SIZE, \n","    sort=True,\n","    sort_key=lambda x: len(x.message), # sort texts by length so that there are sentences with the same length next to each other and less padding is added\n","    repeat=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2JtvOuVbT3i7"},"source":["Let's take a look inside the batch"]},{"cell_type":"code","source":["next(iter(test_iterator))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tFtoGHPggE3K","executionInfo":{"status":"ok","timestamp":1665823081631,"user_tz":-180,"elapsed":5,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"296d6fca-35d2-4808-8347-d5c9310ea482"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","[torchtext.legacy.data.batch.Batch of size 128]\n","\t[.message]:[torch.LongTensor of size 8x128]\n","\t[.category]:[torch.LongTensor of size 128]"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","metadata":{"id":"UQuNMEB5Tgmz"},"source":["for i, batch in enumerate(test_iterator):\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_kE9EDfGT_Gr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665823082025,"user_tz":-180,"elapsed":13,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"1980ab55-3036-46dc-fd9b-2c94b1b56a7e"},"source":["batch.fields"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['message', 'category'])"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","metadata":{"id":"9Kvf2Hf2UEtR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665823082408,"user_tz":-180,"elapsed":5,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"73783a73-9fdc-401b-c7ad-44316da12640"},"source":["batch.batch_size"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["65"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","metadata":{"id":"l1oLPN8-TuA_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665823091724,"user_tz":-180,"elapsed":384,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"be0e46c6-7520-4cc3-db29-2a72b008d1db"},"source":["batch.message"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[  522, 14487,   790,  ...,   539,    18,    12],\n","        [   20,   409,     2,  ...,     5,  4320,   805],\n","        [  139,    41,   254,  ...,     3,  2905,   263],\n","        ...,\n","        [ 1103,     1,     1,  ...,     1,     1,     1],\n","        [ 1398,     1,     1,  ...,     1,     1,     1],\n","        [    4,     1,     1,  ...,     1,     1,     1]])"]},"metadata":{},"execution_count":71}]},{"cell_type":"code","metadata":{"id":"8_TjGTb0UHDE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665823092049,"user_tz":-180,"elapsed":11,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"7cf37a1d-fbb4-44f6-b7de-0471f96aa07e"},"source":["batch.category"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2,\n","        1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1,\n","        1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1])"]},"metadata":{},"execution_count":72}]},{"cell_type":"markdown","metadata":{"id":"y7i8JbEnULFC"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"RTPx1IJ2SlXf"},"source":["### Model"]},{"cell_type":"code","metadata":{"id":"jRr8D0t3Sl1a"},"source":["import torch.nn as nn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UHEDOjrAFhP5"},"source":["We will use nn.Conv2d to create a convolutional layer. in our case `in_channels` is one (text), `out_channels` is the number of filters and the size of the kernels of all filters. Each filter will have a dimension [n x embedding dimension], where n is the size of the n-gram being processed.\n","\n","It is important that the sentences were at least as long as the size of the largest filter used (this is not a problem our case since the dataset doesn't contain texts consisting of five or less words). \n","\n","The generalized model is [as follows](https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f):\n","![](https://dl.dropboxusercontent.com/s/3fzyt3hc5t01ogz/0_0efgxnFIaLTZ2qkY.png)\n","\n","\n","You can use pretrained embeddings with `nn`.`Embedding`.`from_pretrained` metod. See detains at [torch manual](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html). <br />\n","Some layers can be frozen with `freeze` = `True` flag."]},{"cell_type":"code","metadata":{"id":"yb5g3czaFhP6"},"source":["class CNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout_proba):\n","        super().__init__()\n","        \n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)        \n","        self.conv_0 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[0], embedding_dim))\n","        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[1], embedding_dim))\n","        self.conv_2 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[2], embedding_dim))\n","        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n","        self.dropout = nn.Dropout(dropout_proba)\n","        \n","    def forward(self, x):\n","        #x = [sent len, batch size]\n","        x = x.permute(1, 0)\n","                \n","        #x = [batch size, sent len]\n","        embedded = self.embedding(x)\n","                \n","        #embedded = [batch size, sent len, emb dim]\n","        embedded = embedded.unsqueeze(1)\n","        \n","        #embedded = [batch size, 1, sent len, emb dim]\n","        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n","        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n","        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n","            \n","        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n","        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n","        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n","        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n","        \n","        #pooled_n = [batch size, n_filters]\n","        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n","\n","        #cat = [batch size, n_filters * len(filter_sizes)]\n","        return self.fc(cat)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_b_kVC1B_0t9"},"source":["Small hint on `squeeze` and `unsqueeze`:\n","![](https://dl.dropboxusercontent.com/s/nriccblve6aanun/NiJu4.png)"]},{"cell_type":"markdown","metadata":{"id":"e_xPva9zFhP8"},"source":["Now we can only use three different filters, but we can create more. In general, you can use `nn.ModuleList` to create layers as a list and make filters based on the number of elements in filter_sizes. [(Like here).](Https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"FenASHjKUb3p"},"source":["### Supplementary functions"]},{"cell_type":"markdown","metadata":{"id":"yf2viKyOE5FF"},"source":["Let us describe the function for accuracy calculation, as well as the functions for train and evaluation of the network:\n","\n","![](https://dl.dropboxusercontent.com/s/xeo8pjg4oponfp1/800px-Preventive_Medicine_-_Statistics_Sensitivity_TPR%2C_Specificity_TNR%2C_PPV%2C_NPV%2C_FDR%2C_FOR%2C_ACCuracy%2C_Likelihood_Ratio%2C_Diagnostic_Odds_Ratio_2_Final_wiki.png)"]},{"cell_type":"code","metadata":{"id":"VQo4PuPzFhPE"},"source":["import torch.nn.functional as F\n","\n","def binary_accuracy(preds, y):\n","    rounded_preds = torch.round(F.sigmoid(preds))\n","    correct = (rounded_preds == y).float()\n","    acc = correct.sum() / len(correct)\n","    return acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jepSRNlkFhPI"},"source":["def train_func(model, iterator, optimizer, criterion):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train()\n","    \n","    for batch in iterator:\n","        optimizer.zero_grad()\n","        \n","        predictions = model(batch.message.cuda()).squeeze(1)\n","\n","        loss = criterion(predictions.float(), batch.category.float().cuda())\n","        acc = binary_accuracy(predictions.float(), batch.category.float().cuda())\n","        \n","        loss.backward()\n","        optimizer.step()\n","        \n","        epoch_loss += loss\n","        epoch_acc += acc\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nb0KPBl8FhPL"},"source":["def evaluate_func(model, iterator, criterion):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","        for batch in iterator:\n","            predictions = model(batch.message.cuda()).squeeze(1)\n","\n","            loss = criterion(predictions.float(), batch.category.float().cuda())\n","            acc = binary_accuracy(predictions.float(), batch.category.float().cuda())\n","\n","            epoch_loss += loss\n","            epoch_acc += acc\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T6CzZJecUpN7"},"source":["### Training preparation"]},{"cell_type":"code","metadata":{"id":"4Q_Yvs_gFhQB"},"source":["INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","N_FILTERS = 100\n","FILTER_SIZES = [3,4,5]\n","OUTPUT_DIM = 1\n","DROPOUT_PROBA = 0.5\n","\n","model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT_PROBA)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8-5mPhI4U0nk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665823101759,"user_tz":-180,"elapsed":367,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"158dab2a-f9e0-4158-c7b1-dbbce1f2e0f4"},"source":["model # let's look at the model again"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CNN(\n","  (embedding): Embedding(20063, 100)\n","  (conv_0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n","  (conv_1): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n","  (conv_2): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n","  (fc): Linear(in_features=300, out_features=1, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")"]},"metadata":{},"execution_count":79}]},{"cell_type":"markdown","metadata":{"id":"lVfQU3wUU4BY"},"source":["Copy downloaded word embeddings to the parameters of the `Embedding` layer, so that you don't need to train it from the very beginning."]},{"cell_type":"code","metadata":{"id":"18Vk11CaZG-u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665823103103,"user_tz":-180,"elapsed":8,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"a4228bd1-2b3f-487f-a634-fe036c267c5e"},"source":["pretrained_embeddings = TEXT.vocab.vectors\n","model.embedding.weight.data.copy_(pretrained_embeddings)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [-0.1077,  0.1105,  0.5981,  ..., -0.8316,  0.4529,  0.0826],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"]},"metadata":{},"execution_count":80}]},{"cell_type":"code","metadata":{"id":"1Yqwwc5rBc5L"},"source":["import torch.optim as optim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"np-BTnydFhQF"},"source":["optimizer = optim.Adam(model.parameters()) # we have given all parameters to the optimizer, so embeddigs will also be fitted\n","criterion = nn.BCEWithLogitsLoss() # binary cross-entropy with logits\n","\n","model = model.cuda() # we will train on gpu! =)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AmWIaqIbFhQF"},"source":["### Training!\n","\n","Using the previously defined functions, let's start training with the Adam optimizer and evaluate the quality on validation and test:"]},{"cell_type":"code","source":["print('0w')"],"metadata":{"id":"40BXsFofj-Mz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665823106468,"user_tz":-180,"elapsed":8,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"7b016ae7-0f17-455e-d214-f3c27f0b3471"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0w\n"]}]},{"cell_type":"code","metadata":{"id":"XZC7S33pFhQH","colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"status":"error","timestamp":1665823107591,"user_tz":-180,"elapsed":8,"user":{"displayName":"Михаил Яковлев","userId":"08377244301379534836"}},"outputId":"b9239a2c-4349-402b-bb15-b3623291ca16"},"source":["N_EPOCHS = 5\n","\n","for epoch in range(N_EPOCHS):\n","    train_loss, train_acc = train_func(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate_func(model, valid_iterator, criterion)\n","    \n","    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-84-88868802ba85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-76-7b87afaad10a>\u001b[0m in \u001b[0;36mtrain_func\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-74-126f2bfe67ce>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#embedded = [batch size, 1, sent len, emb dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mconved_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mconved_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mconved_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED"]}]},{"cell_type":"code","metadata":{"id":"bXQYQCLCFhQJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637254650636,"user_tz":-180,"elapsed":3504,"user":{"displayName":"Ilia Karpov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuUGLKTmXxaf0Fm2e1dZiBxMh50QwDDPbkGd6m=s64","userId":"09576372577645036195"}},"outputId":"76b06e27-1927-4c14-af73-5d19e43dc606"},"source":["test_loss , test_acc = evaluate_func(model, test_iterator, criterion)\n","print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 0.303, Test Acc: 89.35%\n"]}]},{"cell_type":"markdown","metadata":{"id":"Mm5CmdjtY6Zj"},"source":["#### Exercise 1: How did embeddings change?\n","\n","Let's check if there have been any significant changes in the relationship between words."]},{"cell_type":"code","metadata":{"id":"xbAon9WMY61-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637254650640,"user_tz":-180,"elapsed":63,"user":{"displayName":"Ilia Karpov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuUGLKTmXxaf0Fm2e1dZiBxMh50QwDDPbkGd6m=s64","userId":"09576372577645036195"}},"outputId":"aad478a7-043c-4353-fa22-97c286eeba13"},"source":["TEXT.vocab.vectors # old embeddings"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n","        ...,\n","        [ 0.4413,  0.3325,  0.1120,  ..., -0.0686,  0.4374,  0.8717],\n","        [ 0.1177,  0.1141,  0.2218,  ..., -1.0694,  0.4712, -0.7554],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"Oej9zvPYarQK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637254650641,"user_tz":-180,"elapsed":46,"user":{"displayName":"Ilia Karpov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuUGLKTmXxaf0Fm2e1dZiBxMh50QwDDPbkGd6m=s64","userId":"09576372577645036195"}},"outputId":"86f84a47-bb67-4b3e-9c15-0d2f3e3e05fa"},"source":["model.embedding.weight.data # new emdeddings"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1.1484e-01,  6.4215e-04,  1.2662e-01,  ..., -8.9887e-02,\n","          2.6266e-01, -8.0051e-02],\n","        [ 5.7736e-02,  5.3258e-02, -1.6023e-01,  ...,  4.4047e-02,\n","          4.6871e-02,  1.6651e-02],\n","        [ 8.3157e-02, -2.8306e-01,  7.3483e-01,  ..., -6.0376e-02,\n","          8.1885e-01,  1.9318e-01],\n","        ...,\n","        [ 3.7476e-01,  4.2159e-01,  5.2185e-02,  ..., -2.0774e-01,\n","          3.8798e-01,  1.0139e+00],\n","        [ 2.3889e-01,  1.2898e-01,  3.3101e-01,  ..., -1.0676e+00,\n","          4.7692e-01, -8.1070e-01],\n","        [-1.3223e-02, -3.9290e-02, -6.3443e-02,  ..., -5.5649e-02,\n","          6.5061e-03,  2.2216e-03]], device='cuda:0')"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"k8mHfM5MbhSx"},"source":["from sklearn.metrics.pairwise import cosine_similarity"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qcFo6zIfbVvL"},"source":["i1, i2 = TEXT.vocab.stoi['perfect'], TEXT.vocab.stoi['awful']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"17bFXBwhbliQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637254654264,"user_tz":-180,"elapsed":247,"user":{"displayName":"Ilia Karpov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuUGLKTmXxaf0Fm2e1dZiBxMh50QwDDPbkGd6m=s64","userId":"09576372577645036195"}},"outputId":"41dc9cd9-044d-4331-c165-442bc5a63758"},"source":["cosine_similarity([\n","  TEXT.vocab.vectors[i1].cpu().numpy(),\n","  TEXT.vocab.vectors[i2].cpu().numpy()\n","  ])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.9999999, 0.5248411],\n","       [0.5248411, 0.9999996]], dtype=float32)"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"YUFaea6bbw8f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637254654265,"user_tz":-180,"elapsed":22,"user":{"displayName":"Ilia Karpov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuUGLKTmXxaf0Fm2e1dZiBxMh50QwDDPbkGd6m=s64","userId":"09576372577645036195"}},"outputId":"200ff9e9-91c7-4982-b3bd-06e9731b9fc8"},"source":["cosine_similarity([\n","  model.embedding.weight.data[i1].cpu().numpy(),\n","  model.embedding.weight.data[i2].cpu().numpy()\n","  ])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1.0000002 , 0.39539462],\n","       [0.39539462, 1.        ]], dtype=float32)"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"u0XE_D_MWE22"},"source":["\"perfect\" and \"awful\" are further from each other now.\n","\n","**Task**: Look at the other changes and try to explain them. You can make a visualization using t-sne for clarity."]},{"cell_type":"code","metadata":{"id":"aVvpv9w-WFOc"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ICox-i-WYYPq"},"source":["#### Excersise 2: nn.ModuleList\n","\n","You can easily define as many different convolutions as you like using nn.ModuleList! Here's an example:"]},{"cell_type":"code","metadata":{"id":"eKCFgWKdXeyO"},"source":["class CNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n","                 dropout, pad_idx):\n","        \n","        super().__init__()\n","                \n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n","        \n","        self.convs = nn.ModuleList([\n","                                    nn.Conv2d(in_channels = 1, \n","                                              out_channels = n_filters, \n","                                              kernel_size = (fs, embedding_dim)) \n","                                    for fs in filter_sizes\n","                                    ])\n","        \n","        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, text):\n","                \n","        #text = [batch size, sent len]\n","        \n","        embedded = self.embedding(text)\n","                \n","        #embedded = [batch size, sent len, emb dim]\n","        \n","        embedded = embedded.unsqueeze(1)\n","        \n","        #embedded = [batch size, 1, sent len, emb dim]\n","        \n","        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n","            \n","        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n","                \n","        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n","        \n","        #pooled_n = [batch size, n_filters]\n","        \n","        cat = self.dropout(torch.cat(pooled, dim = 1))\n","\n","        #cat = [batch size, n_filters * len(filter_sizes)]\n","            "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MSb06TT3cwO9"},"source":["**Task**: experiment with the number and size of the bundles. Which works best?"]},{"cell_type":"code","metadata":{"id":"eO5TShOrc5OV"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GGNUN9TTYnxO"},"source":["#### Exercise 3: Another preprocessing\n","\n","We used `data.Field (tokenize = 'spacy')` when loading data.\n","Let's try to replace the `spacy` tokenizer with our own function, which additionally cleans data from garbage."]},{"cell_type":"code","metadata":{"id":"YkmL4MxZZ1N0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637254654267,"user_tz":-180,"elapsed":20,"user":{"displayName":"Ilia Karpov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuUGLKTmXxaf0Fm2e1dZiBxMh50QwDDPbkGd6m=s64","userId":"09576372577645036195"}},"outputId":"d702f61d-d7ec-49c8-b0ed-dc431416295b"},"source":["# пример мусора\n","ds.examples[0].text[25:40]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['is',\n"," 'exactly',\n"," 'what',\n"," 'happened',\n"," 'with',\n"," 'me.<br',\n"," '/><br',\n"," '/>The',\n"," 'first',\n"," 'thing',\n"," 'that',\n"," 'struck',\n"," 'me',\n"," 'about',\n"," 'Oz']"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"yRV729TtZfxC"},"source":["Preprocessing (from the last workshop):"]},{"cell_type":"code","metadata":{"id":"bnDJ5sFwZfxD"},"source":["from bs4 import BeautifulSoup\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iL2EXExRZfxF"},"source":["def review_to_wordlist(review):\n","    # remove links\n","    review = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \" \", review)\n","    # get the text\n","    review_text = BeautifulSoup(review, \"lxml\").get_text()\n","    # keep only word symbols\n","    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n","    # convert words to lowercase and split into words by space character\n","    return review_text.lower().split()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xXbasRI5dBfM"},"source":["**Task**: Try to train the model using a different preprocessing. Has it gotten better? What if we remove the stop words?"]},{"cell_type":"code","metadata":{"id":"L4PiG-hIZfxH"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z9ICtMyetR_G"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sfuamkw28rgp"},"source":["# Data Augmentation"]},{"cell_type":"markdown","metadata":{"id":"89zDRG-jIdNB"},"source":["In our example, the data was balanced, but how to deal with unbalanced data?\n","\n","Consider the problem of recognizing the sentiment of tweets taken from the [Twitter Sentimental Analysis challenge](https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/).\n","\n","Presentation source: https://github.com/mabusalah/Resampling"]},{"cell_type":"markdown","metadata":{"id":"uLKzWFu-QWAm"},"source":["Downoad the data"]},{"cell_type":"code","metadata":{"id":"5_WpbqWhk-a5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637254658876,"user_tz":-180,"elapsed":4381,"user":{"displayName":"Ilia Karpov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuUGLKTmXxaf0Fm2e1dZiBxMh50QwDDPbkGd6m=s64","userId":"09576372577645036195"}},"outputId":"2eccbadd-1cda-42b4-e224-952ea52f6738"},"source":["!wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=1Jjuk23nMTQkfA3-3_HpevXGeupav7QLz\" -O train.csv\n","!wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=11FugxTRrdKqkDE_3KlfCDWRn_rbR6VxM\" -O test.csv"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-11-18 16:57:34--  https://drive.google.com/uc?export=download&id=1Jjuk23nMTQkfA3-3_HpevXGeupav7QLz\n","Resolving drive.google.com (drive.google.com)... 142.250.136.100, 142.250.136.101, 142.250.136.113, ...\n","Connecting to drive.google.com (drive.google.com)|142.250.136.100|:443... connected.\n","HTTP request sent, awaiting response... 302 Moved Temporarily\n","Location: https://doc-0s-44-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/o1rp67ts6kcuospsjg7earbpfbb0frmj/1637254650000/13414369628864094336/*/1Jjuk23nMTQkfA3-3_HpevXGeupav7QLz?e=download [following]\n","Warning: wildcards not supported in HTTP.\n","--2021-11-18 16:57:35--  https://doc-0s-44-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/o1rp67ts6kcuospsjg7earbpfbb0frmj/1637254650000/13414369628864094336/*/1Jjuk23nMTQkfA3-3_HpevXGeupav7QLz?e=download\n","Resolving doc-0s-44-docs.googleusercontent.com (doc-0s-44-docs.googleusercontent.com)... 74.125.129.132, 2607:f8b0:4001:c15::84\n","Connecting to doc-0s-44-docs.googleusercontent.com (doc-0s-44-docs.googleusercontent.com)|74.125.129.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3103165 (3.0M) [text/csv]\n","Saving to: ‘train.csv’\n","\n","train.csv           100%[===================>]   2.96M  5.15MB/s    in 0.6s    \n","\n","2021-11-18 16:57:36 (5.15 MB/s) - ‘train.csv’ saved [3103165/3103165]\n","\n","--2021-11-18 16:57:36--  https://drive.google.com/uc?export=download&id=11FugxTRrdKqkDE_3KlfCDWRn_rbR6VxM\n","Resolving drive.google.com (drive.google.com)... 142.250.136.100, 142.250.136.101, 142.250.136.113, ...\n","Connecting to drive.google.com (drive.google.com)|142.250.136.100|:443... connected.\n","HTTP request sent, awaiting response... 302 Moved Temporarily\n","Location: https://doc-04-44-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/n628eoa6gq0rqk01r3g8d1e4a4f4hk6p/1637254650000/13414369628864094336/*/11FugxTRrdKqkDE_3KlfCDWRn_rbR6VxM?e=download [following]\n","Warning: wildcards not supported in HTTP.\n","--2021-11-18 16:57:37--  https://doc-04-44-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/n628eoa6gq0rqk01r3g8d1e4a4f4hk6p/1637254650000/13414369628864094336/*/11FugxTRrdKqkDE_3KlfCDWRn_rbR6VxM?e=download\n","Resolving doc-04-44-docs.googleusercontent.com (doc-04-44-docs.googleusercontent.com)... 74.125.129.132, 2607:f8b0:4001:c15::84\n","Connecting to doc-04-44-docs.googleusercontent.com (doc-04-44-docs.googleusercontent.com)|74.125.129.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1635543 (1.6M) [text/csv]\n","Saving to: ‘test.csv’\n","\n","test.csv            100%[===================>]   1.56M  2.10MB/s    in 0.7s    \n","\n","2021-11-18 16:57:38 (2.10 MB/s) - ‘test.csv’ saved [1635543/1635543]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"ILRsSOuWJGFP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637254658877,"user_tz":-180,"elapsed":37,"user":{"displayName":"Ilia Karpov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuUGLKTmXxaf0Fm2e1dZiBxMh50QwDDPbkGd6m=s64","userId":"09576372577645036195"}},"outputId":"bd4a3dd9-1385-49b9-a2f0-a665ae05460f"},"source":["import pandas as pd\n","test = pd.read_csv('test.csv')\n","print(\"Test Set:\"% test.columns, test.shape, len(test))\n","train = pd.read_csv('train.csv')\n","print(\"Training Set:\"% train.columns, train.shape, len(train))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Set: (17197, 2) 17197\n","Training Set: (31962, 3) 31962\n"]}]},{"cell_type":"code","metadata":{"id":"sBtAsQDNJPXX","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1637254658879,"user_tz":-180,"elapsed":31,"user":{"displayName":"Ilia Karpov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuUGLKTmXxaf0Fm2e1dZiBxMh50QwDDPbkGd6m=s64","userId":"09576372577645036195"}},"outputId":"eb98e0c5-1133-416a-b1cc-e03c262d304f"},"source":["train.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>@user when a father is dysfunctional and is s...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>@user @user thanks for #lyft credit i can't us...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>bihday your majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>#model   i love u take with u all the time in ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>factsguide: society now    #motivation</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  label                                              tweet\n","0   1      0   @user when a father is dysfunctional and is s...\n","1   2      0  @user @user thanks for #lyft credit i can't us...\n","2   3      0                                bihday your majesty\n","3   4      0  #model   i love u take with u all the time in ...\n","4   5      0             factsguide: society now    #motivation"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","metadata":{"id":"XV-4lGQVJRc2","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1637254658881,"user_tz":-180,"elapsed":31,"user":{"displayName":"Ilia Karpov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuUGLKTmXxaf0Fm2e1dZiBxMh50QwDDPbkGd6m=s64","userId":"09576372577645036195"}},"outputId":"4ce1a06d-af5f-4649-eb99-570708d9bce3"},"source":["test.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>31963</td>\n","      <td>#studiolife #aislife #requires #passion #dedic...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>31964</td>\n","      <td>@user #white #supremacists want everyone to s...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>31965</td>\n","      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>31966</td>\n","      <td>is the hp and the cursed child book up for res...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>31967</td>\n","      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      id                                              tweet\n","0  31963  #studiolife #aislife #requires #passion #dedic...\n","1  31964   @user #white #supremacists want everyone to s...\n","2  31965  safe ways to heal your #acne!!    #altwaystohe...\n","3  31966  is the hp and the cursed child book up for res...\n","4  31967    3rd #bihday to my amazing, hilarious #nephew..."]},"metadata":{},"execution_count":55}]},{"cell_type":"markdown","metadata":{"id":"xarXsO4DJU8Y"},"source":["Let us see the percentage of the total samples in positive and negative examples."]},{"cell_type":"code","metadata":{"id":"eT7Rz3MNJTVP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637254658882,"user_tz":-180,"elapsed":30,"user":{"displayName":"Ilia Karpov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuUGLKTmXxaf0Fm2e1dZiBxMh50QwDDPbkGd6m=s64","userId":"09576372577645036195"}},"outputId":"0039d94f-6669-41b5-d995-b64e9a92b45e"},"source":["print(\"Positive: \", train.label.value_counts()[0]/len(train)*100,\"%\")\n","print(\"Negative: \", train.label.value_counts()[1]/len(train)*100,\"%\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Positive:  92.98542018647143 %\n","Negative:  7.014579813528565 %\n"]}]},{"cell_type":"markdown","metadata":{"id":"VB6Jaz-mJ1zR"},"source":["93% vs. 7% - the data is definitely unbalanced, which, in turn, negatively affects the accuracy of the prediction.\n","First, let's work with the initial data and evaluate the classification accuracy. Let's start with data preprocessing: remove numbers, html / xml tags, special characters from tweets."]},{"cell_type":"code","metadata":{"id":"FCmZHO1kJfpP"},"source":["import re\n","from bs4 import BeautifulSoup #handling html/xml tags\n","from nltk.tokenize import WordPunctTokenizer\n","from nltk.stem import PorterStemmer\n","\n","porter=PorterStemmer()\n","tok = WordPunctTokenizer()\n","pat1 = r'@[A-Za-z0-9]+'\n","pat2 = r'https?://[A-Za-z0-9./]+'\n","combined_pat = r'|'.join((pat1, pat2))\n","\n","def tweet_cleaner(text):\n","    soup = BeautifulSoup(text, 'lxml')\n","    souped = soup.get_text()\n","    stripped = re.sub(combined_pat, '', souped)\n","    try:\n","        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n","    except:\n","        clean = stripped\n","    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n","    lower_case = letters_only.lower()\n","\n","    words = tok.tokenize(lower_case)\n","    \n","    stem_sentence=[]\n","    for word in words:\n","        stem_sentence.append(porter.stem(word))\n","        stem_sentence.append(\" \")\n","    words=\"\".join(stem_sentence).strip()\n","    return words\n","\n","nums = [0,len(train)]\n","clean_tweet_texts = []\n","for i in range(nums[0],nums[1]):\n","    clean_tweet_texts.append(tweet_cleaner(train['tweet'][i]))\n","    \n","nums = [0,len(test)]\n","test_tweet_texts = []\n","\n","for i in range(nums[0],nums[1]):\n","    test_tweet_texts.append(tweet_cleaner(test['tweet'][i])) \n","    \n","train_clean = pd.DataFrame(clean_tweet_texts,columns=['tweet'])\n","train_clean['label'] = train.label\n","train_clean['id'] = train.id\n","test_clean = pd.DataFrame(test_tweet_texts,columns=['tweet'])\n","test_clean['id'] = test.id"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lkQ5fDVUM9EU"},"source":["Let's divide the data into training and test data."]},{"cell_type":"code","metadata":{"id":"eGEVUF2u29sL"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d23SW3-tMdKk"},"source":["from sklearn import model_selection, preprocessing, metrics, linear_model, svm\n","\n","train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_clean['tweet'],train_clean['label'])\n","encoder = preprocessing.LabelEncoder()\n","train_y = encoder.fit_transform(train_y)\n","valid_y = encoder.fit_transform(valid_y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tfj1ZjQANVSK"},"source":["Let's calculate TF-IDF weights."]},{"cell_type":"code","metadata":{"id":"RNw9U1WnNKfq"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","\n","tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=100000)\n","tfidf_vect.fit(train_clean['tweet'])\n","xtrain_tfidf =  tfidf_vect.transform(train_x)\n","xvalid_tfidf =  tfidf_vect.transform(valid_x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K8TxxultNlOq"},"source":["Accuracy metric works well only for balanced datasets, so we will use the F1 measure to evaluate the results of the algorithm."]},{"cell_type":"code","metadata":{"id":"cx29ZstgNXqb"},"source":["def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n","    classifier.fit(feature_vector_train, label)\n","\n","    predictions = classifier.predict(feature_vector_valid)    \n","\n","    return metrics.f1_score(valid_y,predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_YEjiEUDNrLj"},"source":["First, let's train log regression."]},{"cell_type":"code","metadata":{"id":"BG3DifE-Nn-I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637254822058,"user_tz":-180,"elapsed":18571,"user":{"displayName":"Ilia Karpov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuUGLKTmXxaf0Fm2e1dZiBxMh50QwDDPbkGd6m=s64","userId":"09576372577645036195"}},"outputId":"461dd14c-99de-49d1-d7c9-84eccc2e01ae"},"source":["accuracyORIGINAL = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),xtrain_tfidf, train_y, xvalid_tfidf)\n","print (\"Logistic regression Baseline, WordLevel TFIDF: \", accuracyORIGINAL)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic regression Baseline, WordLevel TFIDF:  0.5257595772787318\n"]}]},{"cell_type":"markdown","metadata":{"id":"rIaiuRviW5pS"},"source":["Try using word count vectorizer for feature extraction."]},{"cell_type":"code","metadata":{"id":"Vsr0wT4NW2LW"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0wGw5DV-3nHf"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fo6tGAtS3nN8"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X0baBnaYN7a0"},"source":["As you can see, we obtain poor result.\n","\n","What can be done with the data?\n","\n","It would be nice to somehow increase the number of negative examples, or reduce the number of positive ones. There are various data augmentation techniques for this. Python has imblearn library (imbalanced-learn) for this purpose."]},{"cell_type":"code","metadata":{"id":"F6zNqmnUN1WB"},"source":["from imblearn.over_sampling import BorderlineSMOTE, SMOTE, ADASYN, SMOTENC, RandomOverSampler\n","from imblearn.under_sampling import (RandomUnderSampler, \n","                                    NearMiss, \n","                                    InstanceHardnessThreshold,\n","                                    CondensedNearestNeighbour,\n","                                    EditedNearestNeighbours,\n","                                    RepeatedEditedNearestNeighbours,\n","                                    AllKNN,\n","                                    NeighbourhoodCleaningRule,\n","                                    OneSidedSelection,\n","                                    TomekLinks)\n","from imblearn.combine import SMOTEENN, SMOTETomek\n","from imblearn.pipeline import make_pipeline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"efoIi6euYMQs"},"source":["Consider using under-sampling, over-sampling and their combination for augmentation."]},{"cell_type":"markdown","metadata":{"id":"DqhD2dodYwJl"},"source":["**Under-sampling** balances the data by reducing the size of the prevailing class.\n","It is reasonable to use this method when the amount of data is large enough, otherwise there is a risk of being left without training examples at all.\n","\n","So, the logic of the action is quite simple: we just randomly remove unnecessary instances from the prevailing class.\n","\n","Since in our example only 7% of all tweets are negative, balancing a positive set with this 7% is unlikely to provide a good result.\n","\n","Let's try ..."]},{"cell_type":"code","metadata":{"id":"AQVfIvf0aTrp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637254824126,"user_tz":-180,"elapsed":2088,"user":{"displayName":"Ilia Karpov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuUGLKTmXxaf0Fm2e1dZiBxMh50QwDDPbkGd6m=s64","userId":"09576372577645036195"}},"outputId":"d86a83d9-9ba9-49ea-8058-4366947455fa"},"source":["rus = RandomUnderSampler(random_state=0, replacement=True)\n","rus_xtrain_tfidf, rus_train_y = rus.fit_resample(xtrain_tfidf, train_y)\n","accuracyrus = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),rus_xtrain_tfidf, rus_train_y, xvalid_tfidf)\n","print (\"Logistic regressio RUS, WordLevel TFIDF: \", accuracyrus)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic regressio RUS, WordLevel TFIDF:  0.48475289169295477\n"]}]},{"cell_type":"markdown","metadata":{"id":"JnwqxfGeaa9R"},"source":["Indeed, things only got worse.\n","\n","Let's try other **under-sampling** algorithms.\n","\n","For example, **NearMiss**. This algorithm chooses which instances to keep in the prevailing class based on some heuristics. There are three variants of this algorithm:\n","\n","**NearMiss-1** leaves those instances from the prevailing class for which the average distance to * k * nearest neighbors from the minority class will be the smallest.\n","\n","**NearMiss-2** leaves those instances from the prevailing class for which the average distance to * k * the farthest neighbors from the minority class will be the smallest.\n","\n","**NearMiss-3** consists of two steps: first, for each instance, * k * nearest neighbors from the prevailing class are selected from the minority class, then, from the larger class, those instances are selected for which the average distance to * k * nearest neighbors is maximum ...\n","\n","![](https://glemaitre.github.io/imbalanced-learn/_images/sphx_glr_plot_nearmiss_001.png)"]},{"cell_type":"code","metadata":{"id":"QSlgmHmOd9tU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637254848984,"user_tz":-180,"elapsed":24868,"user":{"displayName":"Ilia Karpov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuUGLKTmXxaf0Fm2e1dZiBxMh50QwDDPbkGd6m=s64","userId":"09576372577645036195"}},"outputId":"432cc8c6-1a48-40eb-9a81-0fe4046558f0"},"source":["for sampler in (NearMiss(version=1),NearMiss(version=2),NearMiss(version=3)):\n","    nm_xtrain_tfidf, nm_train_y = sampler.fit_resample(xtrain_tfidf, train_y)\n","    accuracysm = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),nm_xtrain_tfidf, nm_train_y, xvalid_tfidf)\n","    print (\"Logistic regression NearMiss(version= {0}), WordLevel TFIDF: \".format(sampler.version), accuracysm)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic regression NearMiss(version= 1), WordLevel TFIDF:  0.2452245740836345\n","Logistic regression NearMiss(version= 2), WordLevel TFIDF:  0.48668796592119284\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:176: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n","  \"The number of the samples to be selected is larger\"\n"]},{"output_type":"stream","name":"stdout","text":["Logistic regression NearMiss(version= 3), WordLevel TFIDF:  0.2963870177587263\n"]}]},{"cell_type":"markdown","metadata":{"id":"A9nNg2irexcc"},"source":["**Edited Nearest Neighbor (ENN)**\n","\n","ENN removes an element from a larger class if its nearest neighbor has a class other than its own."]},{"cell_type":"code","metadata":{"id":"WFjd6PzjfWEU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637254910828,"user_tz":-180,"elapsed":61858,"user":{"displayName":"Ilia Karpov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuUGLKTmXxaf0Fm2e1dZiBxMh50QwDDPbkGd6m=s64","userId":"09576372577645036195"}},"outputId":"c9328798-65f7-474e-9065-3cbddd711229"},"source":["enn_xtrain_tfidf, enn_train_y = EditedNearestNeighbours().fit_resample(xtrain_tfidf, train_y)\n","accuracy = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),enn_xtrain_tfidf, enn_train_y, xvalid_tfidf)\n","print (\"Logistic regression {0}, WordLevel TFIDF: \", accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic regression {0}, WordLevel TFIDF:  0.5329883570504528\n"]}]},{"cell_type":"markdown","metadata":{"id":"3VcHHV4jgZ50"},"source":["As you can see, applying the **Under-sampling** technique does not generate new data, unlike the **Over-sampling**."]},{"cell_type":"markdown","metadata":{"id":"2CHYj7jW7z5q"},"source":["# Over-sampling"]},{"cell_type":"markdown","metadata":{"id":"ODhcR4G672lq"},"source":["So, when there is not enough data or the number of instances in a minority class is very small, **Over-sampling** is applied.\n","\n","With this technique, data balancing occurs by increasing the number of instances in the minority class. New elements are generated by: repetition, bootstrapping, **SMOTE** (Synthetic Minority Over-Sampling Technique) or **ADASYN** (Adaptive synthetic sampling)."]},{"cell_type":"markdown","metadata":{"id":"Ea9aYY4mxsqo"},"source":["**Random Over-sampling**: randomly duplicates some elements from the minority class."]},{"cell_type":"code","metadata":{"id":"t-6Ivm0ZOCOt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637254935449,"user_tz":-180,"elapsed":24637,"user":{"displayName":"Ilia Karpov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuUGLKTmXxaf0Fm2e1dZiBxMh50QwDDPbkGd6m=s64","userId":"09576372577645036195"}},"outputId":"e51fa918-90f5-483a-cbd0-0f7836cfcf89"},"source":["#Random Over Sampling\n","ros = RandomOverSampler(random_state=777)\n","ros_xtrain_tfidf, ros_train_y = ros.fit_resample(xtrain_tfidf, train_y)\n","accuracyROS = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),ros_xtrain_tfidf, ros_train_y, xvalid_tfidf)\n","print (\"Logistic regression ROS, WordLevel TFIDF: \", accuracyROS)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic regression ROS, WordLevel TFIDF:  0.6534325889164598\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]}]},{"cell_type":"markdown","metadata":{"id":"aXp1gpdDz5p5"},"source":["**SMOTE Over-sampling**\n","\n","The SMOTE algorithm is based on the idea of ​​generating a number of artificial examples that would be “similar” to those in the minority class, but would not duplicate them.\n","\n","To create a new record, find the difference $d=X_b-X_a$, where $X_b,X_a$ - vectors of features of \"neighboring\" examples $a$ and $b$ from the minority class.\n","\n","They are found using the nearest neighbor algorithm (*KNN*). In this case, it is necessary and sufficient for the $b$ example to obtain a set of $k$ neighbors, from which the record $a$ will be selected in the future. The rest of the steps of the *KNN* algorithm are not required.\n","\n","Then, from $d$, by multiplying each of its elements by a random number in the interval (0, 1), $\\hat{d}$ is obtained. The feature vector of the new example is calculated by adding $X_a$ and $\\hat{d}$.\n","\n","The **SMOTE** algorithm allows you to specify the number of records that must be artificially generated. The degree of similarity between the examples $ a $ and $ b $ can be adjusted by changing the value of $ k $ (the number of nearest neighbors).\n","\n","![](https://hsto.org/getpro/habr/post_images/c57/e7e/f4f/c57e7ef4f8711ad2eda881651a027867.png)"]},{"cell_type":"code","metadata":{"id":"HWzqbIWsOEg1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637254950916,"user_tz":-180,"elapsed":15483,"user":{"displayName":"Ilia Karpov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuUGLKTmXxaf0Fm2e1dZiBxMh50QwDDPbkGd6m=s64","userId":"09576372577645036195"}},"outputId":"60a0eec7-3f6e-41eb-f54b-62e4fbd68bf3"},"source":["sm = SMOTE(random_state=777, sampling_strategy = 1.0)\n","sm_xtrain_tfidf, sm_train_y = sm.fit_resample(xtrain_tfidf, train_y)\n","accuracySMOTE = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),sm_xtrain_tfidf, sm_train_y, xvalid_tfidf)\n","print (\"Logistic regression SMOTE, WordLevel TFIDF: \", accuracySMOTE)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic regression SMOTE, WordLevel TFIDF:  0.6571668063704945\n"]}]},{"cell_type":"markdown","metadata":{"id":"9gpJkvdb10Sg"},"source":["So, compared to **Random Over-sampling**, the difference is small.\n","\n","Check **Random Over-sampling** and **SMOTE Over-sampling** results for real test data (*test_clean*)."]},{"cell_type":"code","metadata":{"id":"7V-VoCAO7qw0"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Jz0o7k82YoX"},"source":["The following algorithm is **ASMO: Adaptive synthetic minority oversampling**.\n","\n","Generate artificial records within individual clusters based on all classes. For each example of a minority class, the m nearest neighbors are found, and based on them (as in SMOTE) new records are created.\n","\n","1. If for each $i$ th example of a minority class from $k$ nearest neighbors $g$ ($g\\leq k$) belongs to the majority class, then the dataset is considered \"scattered\". In this case, the **ASMO** algorithm is used, otherwise **SMOTE** is used (as a rule, $g$ is set equal to 20).\n","2. Using only minority class examples, select several clusters (for example, using the $k$ -means algorithm).\n","3. Generate artificial records within individual clusters based on all classes. For each example of a minority class, the m nearest neighbors are found, and based on them (as in **SMOTE**) new records are created.\n","\n","![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQdTzjHBZ_9At5GIDRpF2AAw9hU1jzcVE5uwA&usqp=CAU)\n","\n","This modification of the **SMOTE** algorithm makes it more adaptable to different datasets with unbalanced classes."]},{"cell_type":"code","metadata":{"id":"mClRDnFM1weo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637254962814,"user_tz":-180,"elapsed":10871,"user":{"displayName":"Ilia Karpov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuUGLKTmXxaf0Fm2e1dZiBxMh50QwDDPbkGd6m=s64","userId":"09576372577645036195"}},"outputId":"3db8ba4e-bc72-4a1a-b68d-a49727975549"},"source":["ad = ADASYN(random_state=777, sampling_strategy = 1.0)\n","ad_xtrain_tfidf, ad_train_y = ad.fit_resample(xtrain_tfidf, train_y)\n","accuracyADASYN = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),ad_xtrain_tfidf, ad_train_y, xvalid_tfidf)\n","print (\"Logistic regression ADASYN, WordLevel TFIDF: \", accuracyADASYN)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic regression ADASYN, WordLevel TFIDF:  0.6515028432168968\n"]}]},{"cell_type":"markdown","metadata":{"id":"iHRcvIRP4fx7"},"source":["Let's check it again with real test examples."]},{"cell_type":"code","metadata":{"id":"YTzMbjvnRyFD"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"br9FjH077ix0"},"source":["# Combination of **Under-** and **Over-sampling**"]},{"cell_type":"markdown","metadata":{"id":"_-RCI42f42Hi"},"source":["Possible combinations can be implemented using *imblearn*:\n","\n","1. **SMOTE** + **ENN**\n","2. **SMOTE** + **Tomek Link Removal** (A pair of two nearest neighbors that belong to different classes is called *Tomek link*. Under-sampling is to remove all such elements from the majority class)\n","\n","More details: https://imbalanced-learn.readthedocs.io/en/stable/api.html#module-imblearn.combine"]},{"cell_type":"code","metadata":{"id":"3pg1YBvT4-xP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637255244433,"user_tz":-180,"elapsed":281651,"user":{"displayName":"Ilia Karpov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhuUGLKTmXxaf0Fm2e1dZiBxMh50QwDDPbkGd6m=s64","userId":"09576372577645036195"}},"outputId":"7b98fcea-c20c-4d65-927f-83d0e8997557"},"source":["se = SMOTEENN(random_state=42)\n","se_xtrain_tfidf, se_train_y = se.fit_resample(xtrain_tfidf, train_y)\n","accuracy = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),se_xtrain_tfidf, se_train_y, xvalid_tfidf)\n","print (\"Logistic regression SMOTEENN: \", accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic regression SMOTEENN:  0.4829721362229102\n"]}]},{"cell_type":"markdown","metadata":{"id":"6Gd9dCEd5aDs"},"source":["The first method did not work well. Evaluate the results of the second approach."]},{"cell_type":"code","metadata":{"id":"bkZofkXo5RG2"},"source":[],"execution_count":null,"outputs":[]}]}